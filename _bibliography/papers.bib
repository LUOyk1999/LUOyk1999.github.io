---
---
@inproceedings{
luo2025unlocking,
title={Unlocking the Potential of Classic GNNs for Graph-level Tasks: Simple Architectures Meet Excellence},
author={Yuankai Luo and Lei Shi and Xiao-Ming Wu},
booktitle={The Forty-second International Conference on Machine Learning},
year={2025},
pdf={https://arxiv.org/abs/2502.09263},
code={https://github.com/LUOyk1999/tunedGNN-G},
selected=true,
abbr={ICML 2025},
preview={GNNplus.jpg}
}

@inproceedings{
luo2025node,
title={Node Identifiers: Compact, Discrete Representations for Efficient Graph Learning},
author={Yuankai Luo and Hongkang Li and Qijiong Liu and Lei Shi and Xiao-Ming Wu},
booktitle={The Thirteenth International Conference on Learning Representations},
year={2025},
website={https://iclr.cc/virtual/2025/poster/28071},
pdf={https://openreview.net/forum?id=t9lS1lX9FQ},
code={https://github.com/LUOyk1999/NodeID},
selected=true,
abbr={ICLR 2025},
preview={NodeID.png}
}


@inproceedings{
luo2025beyond,
title={Beyond Random Masking: When Dropout meets Graph Convolutional Networks},
author={Yuankai Luo and Xiao-Ming Wu and Hao Zhu},
booktitle={The Thirteenth International Conference on Learning Representations},
year={2025},
website={https://iclr.cc/virtual/2025/poster/29723},
pdf={https://openreview.net/forum?id=PwxYoMvmvy},
code={https://github.com/LUOyk1999/dropout-theory},
selected=true,
abbr={ICLR 2025},
preview={dropout.png}
}

@inproceedings{
luo2024classic,
title={Classic GNNs are Strong Baselines: Reassessing GNNs for Node Classification},
author={Yuankai Luo and Lei Shi and Xiao-Ming Wu},
booktitle={Thirty-eighth Conference on Neural Information Processing Systems},
year={2024},
website={https://neurips.cc/virtual/2024/poster/97440},
code={https://github.com/LUOyk1999/tunedGNN},
pdf={https://arxiv.org/abs/2406.08993},
selected=true,
abbr={NeurIPS 2024},
preview={GNN2.png}
}

@inproceedings{
luo2024transformers,
title={Enhancing Graph Transformers with Hierarchical Distance Structural Encoding},
author={Yuankai Luo and Hongkang Li and Lei Shi and Xiao-Ming Wu},
booktitle={Thirty-eighth Conference on Neural Information Processing Systems},
year={2024},
website={https://neurips.cc/virtual/2024/poster/94991},
code={https://github.com/LUOyk1999/HDSE},
pdf={https://arxiv.org/abs/2308.11129},
selected=true,
abbr={NeurIPS 2024},
preview={HDSE.png}
}

@inproceedings{
luo2023transformers,
title={Transformers over Directed Acyclic Graphs},
author={Yuankai Luo and Veronika Thost and Lei Shi},
booktitle={Thirty-seventh Conference on Neural Information Processing Systems},
year={2023},
website={https://neurips.cc/virtual/2023/poster/70871},
code={https://github.com/LUOyk1999/DAGformer},
pdf={https://openreview.net/pdf?id=g49s1N5nmO},
poster={https://neurips.cc/media/PosterPDFs/NeurIPS%202023/70871.png?t=1699598473.3900292},
slides={https://neurips.cc/media/neurips-2023/Slides/70871.pdf},
selected=true,
abbr={NeurIPS 2023},
preview={DAG.png}
}

@inproceedings{
luo2023improving,
title={Improving Self-supervised Molecular Representation Learning using Persistent Homology},
author={Yuankai Luo and Lei Shi and Veronika Thost},
booktitle={Thirty-seventh Conference on Neural Information Processing Systems},
year={2023},
website={https://neurips.cc/virtual/2023/poster/70071},
pdf={https://openreview.net/pdf?id=wEiUGpcr0M},
code={https://github.com/LUOyk1999/Molecular-homology},
poster={https://neurips.cc/media/PosterPDFs/NeurIPS%202023/70071.png?t=1699881114.8547847},
slides={https://neurips.cc/media/neurips-2023/Slides/70071.pdf},
selected=true,
abbr={NeurIPS 2023},
preview={PH.png}
}

@inproceedings{10.1145/3580305.3599845,
author = {Luo, Yuankai and Shi, Lei and Xu, Mufan and Ji, Yuwen and Xiao, Fengli and Hu, Chunming and Shan, Zhiguang},
title = {Impact-Oriented Contextual Scholar Profiling Using Self-Citation Graphs},
year = {2023},
isbn = {9798400701030},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
website = {https://doi.org/10.1145/3580305.3599845},
arxiv = {2304.12217},
doi = {10.1145/3580305.3599845},
abstract = {Quantitatively profiling a scholar's scientific impact is important to modern research society. Current practices with bibliometric indicators (e.g., h-index), lists, and networks perform well at scholar ranking, but do not provide structured context for scholar-centric, analytical tasks such as profile reasoning and understanding. This work presents GeneticFlow (GF), a suite of novel graph-based scholar profiles that fulfill three essential requirements: structured-context, scholar-centric, and evolution-rich. We propose a framework to compute GF over large-scale academic data sources with millions of scholars. The framework encompasses a new unsupervised advisor-advisee detection algorithm, a well-engineered citation type classifier using interpretable features, and a fine-tuned graph neural network (GNN) model. Evaluations are conducted on the real-world task of scientific award inference. Experiment outcomes show that the F1 score of best GF profile significantly outperforms alternative methods of impact indicators and bibliometric networks in all the 6 computer science fields considered. Moreover, the core GF profiles, with 63.6\%sim66.5\% nodes and 12.5\%sim29.9\% edges of the full profile, still significantly outrun existing methods in 5 out of 6 fields studied. Visualization of GF profiling result also reveals human explainable patterns for high-impact scholars.},
booktitle = {Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
pages = {4572–4583},
numpages = {12},
keywords = {big scholar data, graph neural networks, scholar profiling},
location = {Long Beach, CA, USA},
series = {KDD '23},
selected=true,
abbr={SIGKDD 2023},
code={https://github.com/LUOyk1999/GeneticFlow},
preview={KDD.png}
}

@article{10.1145/3563457,
author = {Shi, Lei and Luo, Yuankai and Ma, Shuai and Tong, Hanghang and Li, Zhetao and Zhang, Xiatian and Shan, Zhiguang},
title = {Mobility Inference on Long-Tailed Sparse Trajectory},
year = {2023},
issue_date = {February 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {14},
number = {1},
issn = {2157-6904},
website = {https://doi.org/10.1145/3563457},
html = {https://doi.org/10.1145/3563457},
doi = {10.1145/3563457},
abstract = {Analyzing the urban trajectory in cities has become an important topic in data mining. How can we model the human mobility consisting of stay and travel states from the raw trajectory data? How can we infer these mobility states from a single user’s trajectory information? How can we further generalize the mobility inference to the real-world trajectory data that span multiple users and are sparsely sampled over time?In this article, based on formal and rigid definitions of the stay/travel mobility, we propose a single trajectory inference algorithm that utilizes a generic long-tailed sparsity pattern in the large-scale trajectory data. The algorithm guarantees a 100\% precision in the stay/travel inference with a provable lower bound in the recall metric. Furthermore, we design a transformer-like deep learning architecture on the problem of mobility inference from multiple sparse trajectories. Several adaptations from the standard transformer network structure are introduced, including the singleton design to avoid the negative effect of sparse labels in the decoder side, the customized space-time embedding on features of location records, and the mask apparatus at the output side for loss function correction. Evaluations on three trajectory datasets of 40 million urban users validate the performance guarantees of the proposed inference algorithm and demonstrate the superiority of our deep learning model, in comparison to sequence learning methods in the literature. On extremely sparse trajectories, the deep learning model improves from the single trajectory inference algorithm with more than two times of overall and F1 accuracy. The model also generalizes to large-scale trajectory data from different sources with good scalability.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = {jan},
articleno = {18},
numpages = {26},
keywords = {Urban data, transformer, trajectory inference},
abbr={ACM-TIST 2023},
code={https://github.com/LUOyk1999/MobilityInference},
preview={TIST2.png}
}
